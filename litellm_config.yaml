model_list:
  # OpenAI Models
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      rpm: 500
      tpm: 30000
      max_tokens: 4096
      temperature: 0.7
    model_info:
      mode: chat
      supports_vision: true
      supports_function_calling: true
      input_cost_per_token: 0.000005
      output_cost_per_token: 0.000015

  - model_name: gpt-4-turbo
    litellm_params:
      model: openai/gpt-4-turbo
      api_key: os.environ/OPENAI_API_KEY
      rpm: 500
      tpm: 30000
      max_tokens: 4096
      temperature: 0.7
    model_info:
      mode: chat
      supports_vision: true
      supports_function_calling: true
      input_cost_per_token: 0.00001
      output_cost_per_token: 0.00003

  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
      rpm: 3500
      tpm: 90000
      max_tokens: 4096
      temperature: 0.7
    model_info:
      mode: chat
      supports_function_calling: true
      input_cost_per_token: 0.0000005
      output_cost_per_token: 0.0000015

  # DeepSeek Models
  - model_name: deepseek-chat
    litellm_params:
      model: deepseek/deepseek-chat
      api_key: os.environ/DEEPSEEK_API_KEY
      api_base: https://api.deepseek.com
      rpm: 1000
      tpm: 50000
      max_tokens: 4096
      temperature: 0.7
    model_info:
      mode: chat
      supports_function_calling: true
      input_cost_per_token: 0.00000014
      output_cost_per_token: 0.00000028

  - model_name: deepseek-coder
    litellm_params:
      model: deepseek/deepseek-coder
      api_key: os.environ/DEEPSEEK_API_KEY
      api_base: https://api.deepseek.com
      rpm: 1000
      tpm: 50000
      max_tokens: 4096
      temperature: 0.1
    model_info:
      mode: chat
      supports_function_calling: true
      input_cost_per_token: 0.00000014
      output_cost_per_token: 0.00000028

# Router Settings
router_settings:
  routing_strategy: "cost-based-routing"
  fallbacks:
    - ["gpt-4o", "gpt-4-turbo", "deepseek-chat"]
    - ["gpt-3.5-turbo", "deepseek-chat"]
    - ["deepseek-chat", "gpt-3.5-turbo"]
  
  # Cost optimization rules
  cost_routing:
    # Route simple tasks to cheaper models
    simple_tasks:
      models: ["deepseek-chat", "gpt-3.5-turbo"]
      max_cost_per_request: 0.01
    
    # Route complex tasks to premium models
    complex_tasks:
      models: ["gpt-4o", "gpt-4-turbo"]
      max_cost_per_request: 0.10
    
    # Route multi-modal tasks to vision-capable models
    multimodal_tasks:
      models: ["gpt-4o", "gpt-4-turbo"]
      requires_vision: true

# General Settings
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  
  # Logging and monitoring
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]
  
  # Rate limiting
  rpm_limit: 1000
  tpm_limit: 100000
  
  # Caching
  cache:
    type: "redis"
    host: os.environ/REDIS_HOST
    port: 6379
    ttl: 3600

# Task-based routing rules
task_routing:
  blog_generation:
    primary_models: ["gpt-4o", "gpt-4-turbo"]
    fallback_models: ["deepseek-chat"]
    max_cost: 0.10
    
  seo_analysis:
    primary_models: ["gpt-3.5-turbo", "deepseek-chat"]
    fallback_models: ["gpt-4o"]
    max_cost: 0.02
    
  keyword_extraction:
    primary_models: ["deepseek-chat", "gpt-3.5-turbo"]
    fallback_models: ["gpt-4o"]
    max_cost: 0.01
    
  content_formatting:
    primary_models: ["deepseek-chat"]
    fallback_models: ["gpt-3.5-turbo"]
    max_cost: 0.005
    
  image_analysis:
    primary_models: ["gpt-4o"]
    fallback_models: ["gpt-4-turbo"]
    max_cost: 0.15
    requires_vision: true
